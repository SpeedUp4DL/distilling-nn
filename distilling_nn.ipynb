{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "from models.resnet_config import Config\n",
    "from models.resnet_utils import *\n",
    "from models.resnet_utils import _max_pool\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "NUM_CLASSES = 188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _get_filenames_and_classes(dataset_dir):\n",
    "    \"\"\"Returns a list of filenames and inferred class names.\n",
    "    Args:\n",
    "        dataset_dir: A directory containing a set of subdirectories representing\n",
    "        class names. Each subdirectory should contain PNG or JPG encoded images.\n",
    "    Returns:\n",
    "        A list of image file paths, relative to `dataset_dir` and the list of\n",
    "        subdirectories, representing class names.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    directories = []\n",
    "    class_names = []\n",
    "    test_size = []\n",
    "    test_count = []\n",
    "\n",
    "    with open('/data/dict/PE/pe_meta.csv', 'rb') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        flist = []\n",
    "        for row in reader:\n",
    "            #print(row['CLASS'])\n",
    "            if len(class_names) < NUM_CLASSES:\n",
    "                path = os.path.join(dataset_dir, row['Label'])\n",
    "                if os.path.isdir(path):\n",
    "                    directories.append(path)\n",
    "                    class_names.append(row['Label'])\n",
    "                    test_size.append(int(row['Number']))\n",
    "                    test_count.append(0)\n",
    "\n",
    "\n",
    "\n",
    "    train_filenames = []\n",
    "    test_filenames = []\n",
    "    return train_filenames, test_filenames, sorted(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_filenames, testing_filenames, class_names = _get_filenames_and_classes(\"/data/dict/PE/raw\")\n",
    "class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n",
    "print(class_names_to_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inference(x, is_training,\n",
    "              num_classes=25,\n",
    "              num_blocks=[3, 4, 6, 3],\n",
    "              use_bias=False,\n",
    "              bottleneck=True):\n",
    "    c = Config()\n",
    "    c['bottleneck'] = bottleneck\n",
    "    c['is_training'] = tf.convert_to_tensor(is_training,\n",
    "                                            dtype='bool',\n",
    "                                            name='is_training')\n",
    "    c['ksize'] = 3\n",
    "    c['stride'] = 1\n",
    "    c['use_bias'] = use_bias\n",
    "    c['fc_units_out'] = num_classes\n",
    "    c['num_blocks'] = num_blocks\n",
    "    c['stack_stride'] = 2\n",
    "\n",
    "    with tf.variable_scope('scale1'):\n",
    "        c['conv_filters_out'] = 64\n",
    "        c['ksize'] = 7\n",
    "        c['stride'] = 2\n",
    "        x = conv(x, c)\n",
    "        x = bn(x, c)\n",
    "        x = activation(x)\n",
    "\n",
    "    with tf.variable_scope('scale2'):\n",
    "        x = _max_pool(x, ksize=3, stride=2)\n",
    "        c['num_blocks'] = num_blocks[0]\n",
    "        c['stack_stride'] = 1\n",
    "        c['block_filters_internal'] = 64\n",
    "        x = stack(x, c)\n",
    "\n",
    "    with tf.variable_scope('scale3'):\n",
    "        c['num_blocks'] = num_blocks[1]\n",
    "        c['block_filters_internal'] = 128\n",
    "        assert c['stack_stride'] == 2\n",
    "        x = stack(x, c)\n",
    "\n",
    "    with tf.variable_scope('scale4'):\n",
    "        c['num_blocks'] = num_blocks[2]\n",
    "        c['block_filters_internal'] = 256\n",
    "        x = stack(x, c)\n",
    "\n",
    "    with tf.variable_scope('scale5'):\n",
    "        c['num_blocks'] = num_blocks[3]\n",
    "        c['block_filters_internal'] = 512\n",
    "        x = stack(x, c)\n",
    "\n",
    "    x = tf.reduce_mean(x, reduction_indices=[1, 2], name=\"avg_pool\")\n",
    "\n",
    "    if num_classes != None:\n",
    "        with tf.variable_scope('fc'):\n",
    "            x = fc(x, c)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_):\n",
    "    with tf.variable_scope(\"Resnet\"):\n",
    "\n",
    "        logits = inference(input_,\n",
    "                                num_classes=NUM_CLASSES,\n",
    "                                is_training=True,\n",
    "                                bottleneck=False,\n",
    "                                num_blocks=[2, 2, 2, 2])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_label(filenames):\n",
    "    array = np.array([class_names_to_ids[label_string.split('/')[-2]] for label_string in filenames], dtype=np.int64)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_KLD_from_specialist(specialist, target, logits, num_classes, batch_size):\n",
    "    s1 = [47, 166, 85]\n",
    "    dustbin = []\n",
    "    for idx in range(num_classes):\n",
    "        if idx not in specialist:\n",
    "            dustbin.append(idx)\n",
    "    special = tf.constant(specialist, dtype=tf.int64)\n",
    "    dust = tf.constant(dustbin, dtype=tf.int64)\n",
    "\n",
    "    special_batch = tf.nn.embedding_lookup(logits[0], special)\n",
    "    dust_batch = tf.nn.embedding_lookup(logits[0], dust)\n",
    "    dust_batch = tf.reduce_sum(dust_batch)\n",
    "    whole_batch = [tf.concat(0, [special_batch, [dust_batch]])]\n",
    "\n",
    "    for idx in range(1, batch_size):\n",
    "        special_batch = tf.nn.embedding_lookup(logits[idx], special)\n",
    "        dust_batch = tf.nn.embedding_lookup(logits[idx], dust)\n",
    "        dust_batch = tf.reduce_sum(dust_batch)\n",
    "        whole_batch = tf.concat(0, [whole_batch, [tf.concat(0, [special_batch, [dust_batch]])]])\n",
    "\n",
    "    KLD = tf.reduce_mean(target*tf.log(target)-(target*tf.log(whole_batch)), axis=1)\n",
    "    # [batchsize * 1]\n",
    "    return KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, ge, sp1, sp2):\n",
    "\n",
    "    #gen\n",
    "    logits = tf.nn.softmax(logits)\n",
    "    #ge = tf.nn.softmax(ge)\n",
    "    ge = tf.nn.softmax(tf.div(ge, 2))\n",
    "    sp1 = tf.nn.softmax(tf.div(sp1, 2))\n",
    "    sp2 = tf.nn.softmax(tf.div(sp2, 2))\n",
    "\n",
    "    #ce0 = tf.reduce_mean(ge*tf.log(ge) - (ge*tf.log(logits)))\n",
    "    ce0 = tf.reduce_mean(ge*tf.log(ge) - (ge*tf.log(logits)), axis=1)\n",
    "\n",
    "    print(\"ce_0 :\", ce0.get_shape())\n",
    "    ##\n",
    "    s1 = [47, 166, 85]\n",
    "    s2 = [19, 166, 83]\n",
    "    #sp1 - Downloader : 47, Virut : 166, MSIL : 85\n",
    "    #sp2 - Bundler : 19, Virut : 166, LoadMoney : 83\n",
    "\n",
    "    lookup_table = np.zeros((188, 3))\n",
    "    for idx in range(188):\n",
    "        lookup_table[idx][0] = 1\n",
    "    for idx in s1:\n",
    "        lookup_table[idx][1] = 0\n",
    "    for idx in s2:\n",
    "        lookup_table[idx][2] = 0\n",
    "\n",
    "    lookup_table = tf.constant(lookup_table, dtype=tf.float32)\n",
    "\n",
    "    ce1 = get_KLD_from_specialist(specialist = s1, logits = logits, target = sp1, num_classes = NUM_CLASSES, batch_size = BATCH_SIZE)\n",
    "\n",
    "    ce2 = get_KLD_from_specialist(specialist = s2, logits = logits, target = sp2, num_classes = NUM_CLASSES, batch_size = BATCH_SIZE)\n",
    "\n",
    "    ce_list = tf.stack((ce0, ce1, ce2))\n",
    "\n",
    "    #print(\"ce_list :\", ce_list.get_shape())\n",
    "\n",
    "    sp_loss = 0\n",
    "\n",
    "    #print(\"sp_set :\", tf.nn.embedding_lookup(lookup_table, 166).get_shape())\n",
    "\n",
    "    for idx in range(BATCH_SIZE):\n",
    "        gen_predict = tf.arg_max(ge[idx], dimension=0)\n",
    "        sp_set = tf.nn.embedding_lookup(lookup_table, gen_predict)\n",
    "        k = tf.reduce_sum(sp_set)\n",
    "        tmp_loss = tf.reduce_sum(sp_set*tf.transpose(ce_list)[idx])\n",
    "        sp_loss += tmp_loss/k\n",
    "\n",
    "    #sp_loss = (sp_loss/BATCH_SIZE)\n",
    "\n",
    "    cross_entropy_mean = sp_loss\n",
    "\n",
    "    return cross_entropy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_accuracy(logits, labels):\n",
    "    correct_prediction = tf.nn.in_top_k(logits, labels, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuarcy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    filename_queue = tf.train.input_producer(path, num_epochs=1, capacity=1, shuffle=True, seed=0)\n",
    "    gen_queue = tf.train.input_producer(gen, num_epochs=1, capacity=1, shuffle=True, seed=0)\n",
    "    sp1_queue = tf.train.input_producer(sp1, num_epochs=1, capacity=1, shuffle=True, seed=0)\n",
    "    sp2_queue = tf.train.input_producer(sp2, num_epochs=1, capacity=1, shuffle=True, seed=0)\n",
    "\n",
    "    fn = filename_queue.dequeue()\n",
    "    ge = gen_queue.dequeue()\n",
    "\n",
    "    s1 = sp1_queue.dequeue()\n",
    "    s2 = sp2_queue.dequeue()\n",
    "\n",
    "    string_tensor = tf.read_file(fn)\n",
    "\n",
    "    content = tf.decode_raw(string_tensor, tf.uint8)\n",
    "    zero_padding_size = tf.constant(224) - tf.mod(tf.shape(content), tf.constant(224))\n",
    "    zero_padding = tf.zeros(zero_padding_size, dtype=tf.uint8)\n",
    "    content = tf.concat(0, [content, zero_padding])\n",
    "\n",
    "    content = tf.reshape(content, [-1, 224, 1])\n",
    "    content = tf.image.resize_images(content, [224, 224], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    img = tf.div(tf.sub(tf.cast(content, tf.float32), 127.5), 127.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    fn_batch, img_batch, ge_batch, sp1_batch, sp2_batch = tf.train.batch([fn, img, ge, s1, s2], BATCH_SIZE, capacity=1, num_threads=1)\n",
    "    #fn_batch, ge_batch = tf.train.batch([fn, ge], 50, capacity=1, num_threads=1)\n",
    "\n",
    "    label_batch = tf.py_func(get_label, [fn_batch], [tf.int64])[0]\n",
    "\n",
    "    logits = build_model(img_batch)\n",
    "    loss = build_loss(logits, ge_batch, sp1_batch, sp2_batch)\n",
    "    accuracy = build_accuracy(logits, label_batch)\n",
    "    #accuracy = build_accuracy(ge_batch, label_batch)\n",
    "\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "    lr = tf.Variable(0.0001, trainable=False)\n",
    "    opt = tf.train.AdamOptimizer(lr).minimize(loss, global_step)\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess = tf.Session()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"/data/tensorboard_log/dict/dist_0324_2/\", sess.graph)\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    checkpoint_dir = \"/data/dict/checkpoint_dist_0324_2/\"\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print('no checkpoint found...')\n",
    "\n",
    "    summary_iter = 10\n",
    "    save_iter = 100\n",
    "\n",
    "    valacc = []\n",
    "    try:\n",
    "        count = 0\n",
    "        while not coord.should_stop():\n",
    "            #value = sess.run([fn_batch, label_batch, img_batch, ge_batch, sp1_batch, sp2_batch])\n",
    "\n",
    "            _,  _loss, acc, merged, _global_step = sess.run([opt, loss, accuracy, merged_summary, global_step])\n",
    "            print('loss : %g, accuracy : %g'%(_loss, acc))\n",
    "            valacc.append(acc)\n",
    "            #acc, _global_step = sess.run([accuracy, global_step])\n",
    "            #print('accuracy : %g'%(acc))\n",
    "            if _global_step % summary_iter == 0:\n",
    "                writer.add_summary(merged, _global_step)\n",
    "                print('write summary')\n",
    "            if _global_step % save_iter == 0:\n",
    "                saver.save(sess, os.path.join(checkpoint_dir,'model.ckpt'), _global_step)\n",
    "                print('save checkpoint')\n",
    "\n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        saver.save(sess, os.path.join(checkpoint_dir,'model.ckpt'), _global_step)\n",
    "        print('eval accuracy : ',np.mean(valacc))\n",
    "        print(\"End of epochs\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        print(\"count: {}\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
